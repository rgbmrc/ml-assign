{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start importing all the needed Python libraries to perform the Neural Network (NN) training (Tensorflow and Keras) and graphical analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the main functions needed to (A) generate the input data set with the corresponding labels, (B) define, build up and train the chosen Neural Network, and (C) implementing the colormap for the grid-search. \n",
    "\n",
    "For more detail, see the documentation of each function.\n",
    "\n",
    "A variable 'samples' sets the number of trials of the NN training we want to perform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 1\n",
    "\n",
    "\n",
    "def generate(\n",
    "    N, train_frac=1.0, rescale=1.0, offset=0.0, augment_frac=0.0, augment_std=0.0\n",
    "):\n",
    "    \"\"\"\n",
    "    This function generates the input data-set of N 2-dimensional points (x1,x2) in a square box and assoaciate them to a label classifying points inside/outside a triangle.\n",
    "    It defines the fraction of data devoted to the training of the network and eventually the augemtations of such data with noise normally distributed.\n",
    "    Args:\n",
    "        N (int):\n",
    "            Number of generated points\n",
    "        train_frac (float, optional):\n",
    "            Fraction of input data devoted to the network training. Defaults to 1.0.\n",
    "        rescale (float, optional):\n",
    "            Scaling factor applied to input data (x1,x2). Defaults to 1..\n",
    "        offset (float, optional):\n",
    "            Offset applied to input data (x1,x2) after rescaling. Defaults to 0..\n",
    "        augment_frac (float, optional):\n",
    "            Fraction of training data distorted with Gaussian noise without changing labels. Defaults to 0.0.\n",
    "        augment_std (float, optional):\n",
    "            std deviation of the Gaussian noise used for augmentation. Defaults to 0.0.\n",
    "    Returns:\n",
    "        (ndarray, ndarray) :\n",
    "            2D input data points (x1,x2) and the associated true 1D labels (y)\n",
    "\n",
    "    \"\"\"\n",
    "    x = np.random.random((N, 2)) * 100 - 50\n",
    "    y = (x[..., 0] > -20) & (x[..., 1] > -40) & ((x[..., 0] + x[..., 1]) < 40)\n",
    "\n",
    "    x = (x - offset) / rescale\n",
    "\n",
    "    N_train = int(N * train_frac)\n",
    "    N_augment = int(N_train * augment_frac)\n",
    "    x_train = np.pad(x[:N_train], ((0, N_augment), (0, 0)), mode=\"wrap\")\n",
    "    y_train = np.pad(y[:N_train], ((0, N_augment)), mode=\"wrap\")\n",
    "    x_train[:N_augment] += np.random.normal(0.0, augment_std, size=(N_augment, 2))\n",
    "    valid = [a[N_train:] for a in (x, y)]\n",
    "    return (x_train, y_train), valid\n",
    "\n",
    "\n",
    "def train(params):\n",
    "    \"\"\"\n",
    "    This function trains multiple times a Deep Neural Network with a chosen setup given by the parameters in 'params' and save the results of each trial into a dictionary.\n",
    "\n",
    "    Args:\n",
    "        params (dictionary of dictionaries): \n",
    "            It contains all the \"parameters\" to build a single DNN configuration.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        dictionary : \n",
    "            Results (loss function & validation accuracy) at each epoch of the DNN training, for all the investigated configurations.\n",
    "    \"\"\"\n",
    "    samples = params.setdefault(\"samples\", 1)\n",
    "    results = defaultdict(list)\n",
    "\n",
    "    for _ in range(samples):\n",
    "        # input\n",
    "        train, valid = generate(**params[\"input\"])\n",
    "\n",
    "        # model\n",
    "        mod = tf.keras.models.Sequential(**params[\"model\"])\n",
    "\n",
    "        # compile\n",
    "        mod.compile(**params[\"compile\"])\n",
    "\n",
    "        # fit\n",
    "        fit = mod.fit(*train, validation_data=valid, **params[\"fit\"])\n",
    "        for k, v in fit.history.items():\n",
    "            results[k].append(v)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def gridplot(vals, data):\n",
    "    \"\"\"\n",
    "    Plot accuracy gridsearch.\n",
    "    Before plotting averages over realizations.\n",
    "\n",
    "    Args:\n",
    "        vals (dictionary of 2 lists): it contains the values of the x and y axis of the gridsearch\n",
    "        data (ndarray): 2d array containing the validation accuracy predicted from the DNN\n",
    "\n",
    "    Returns:\n",
    "        the colormap describing the grid search of the DNN accuracy in terms two parameters\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    data = np.asarray(data)[..., -1].mean(-1)\n",
    "    data = data.reshape([len(v) for v in vals.values()])\n",
    "    sm = ax.imshow(data, origin=\"lower\", cmap=\"magma\")\n",
    "    for axis, (p, vs) in zip((ax.yaxis, ax.xaxis), vals.items()):\n",
    "        axis.set_major_locator(mpl.ticker.FixedLocator(np.arange(len(vs))))\n",
    "        axis.set_major_formatter(mpl.ticker.FixedFormatter(vs))\n",
    "        axis.set_label_text(p)\n",
    "    cbar = fig.colorbar(sm)\n",
    "    cbar.set_label(\"accuracy\")\n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIMULATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1A) Input Size vs Training fraction\n",
    "\n",
    "We consider different sizes $N$ of the input data set with different fractions $\\chi$ devoted to the training. \n",
    "Namely:\n",
    "\\begin{align*}\n",
    "    N&\\in\\left\\{1000,2000,4000,8000,16000\\right\\}\\\\\n",
    "    \\chi&\\in\\left\\{0.6,0.7,0.8,0.9\\right\\}\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of varying parameters\n",
    "vals = {\n",
    "    \"N\": 1000 * 2 ** np.arange(5),\n",
    "    \"train_frac\": [0.6, 0.7, 0.8, 0.9],\n",
    "}\n",
    "\n",
    "# Dictionary of DNN configuration setups\n",
    "params = [\n",
    "    {\n",
    "        \"samples\": samples,\n",
    "        \"input\": {\n",
    "            \"N\": N,\n",
    "            \"rescale\": 50,\n",
    "            \"offset\": 0,\n",
    "            \"train_frac\": train_frac,\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"name\": \"model\",\n",
    "            \"layers\": [\n",
    "                tf.keras.layers.Dense(2, activation=\"relu\"),\n",
    "                tf.keras.layers.Dense(20, activation=\"relu\"),\n",
    "                tf.keras.layers.Dense(20, activation=\"relu\"),\n",
    "                tf.keras.layers.Dropout(0.2),\n",
    "                tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "            ],\n",
    "        },\n",
    "        \"compile\": {\n",
    "            \"loss\": \"binary_crossentropy\",\n",
    "            \"optimizer\": \"adam\",\n",
    "            \"metrics\": [\"accuracy\"],\n",
    "            \"steps_per_execution\": 4,\n",
    "        },\n",
    "        \"fit\": {\"epochs\": 500, \"batch_size\": 50, \"verbose\": 0},\n",
    "    }\n",
    "    for N, train_frac in product(*vals.values())\n",
    "]\n",
    "fig, ax = gridplot(vals, [train(p)[\"val_accuracy\"] for p in params])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1B) Training fraction vs Augmentation Fraction\n",
    "\n",
    "For a fixed value of $N=4000$, we consider different training fractions $\\chi$ to which we apply augmentation with different percentages $\\eta$. Namely:\n",
    "\\begin{align*}\n",
    "    \\chi&\\in\\left\\{0.6,0.7,0.8,0.9\\right\\}\\\\\n",
    "    \\eta&\\in\\left\\{0.0,0.\\overline{3},0.\\overline{6}\\right\\}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of varying parameters\n",
    "vals = {\n",
    "    \"train_frac\": np.arange(6, 10) / 10,\n",
    "    \"augment_frac\": np.arange(4) / 3,\n",
    "}\n",
    "\n",
    "# Dictionary of DNN configuration setups\n",
    "params = [\n",
    "    {\n",
    "        \"samples\": samples,\n",
    "        \"input\": {\n",
    "            \"N\": 4000,\n",
    "            \"rescale\": 50,\n",
    "            \"offset\": 0,\n",
    "            \"train_frac\": train_frac,\n",
    "            \"augment_frac\": augment_frac,\n",
    "            \"augment_std\": 0.05773502691896,\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"name\": \"model\",\n",
    "            \"layers\": [\n",
    "                tf.keras.layers.Dense(2, activation=\"relu\"),\n",
    "                tf.keras.layers.Dense(20, activation=\"relu\"),\n",
    "                tf.keras.layers.Dense(20, activation=\"relu\"),\n",
    "                tf.keras.layers.Dropout(0.2),\n",
    "                tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "            ],\n",
    "        },\n",
    "        \"compile\": {\n",
    "            \"loss\": \"binary_crossentropy\",\n",
    "            \"optimizer\": \"adam\",\n",
    "            \"metrics\": [\"accuracy\"],\n",
    "            \"steps_per_execution\": 4,\n",
    "        },\n",
    "        \"fit\": {\"epochs\": 500, \"batch_size\": 50, \"verbose\": 0},\n",
    "    }\n",
    "    for train_frac, augment_frac in product(*vals.values())\n",
    "]\n",
    "\n",
    "results = []\n",
    "for p in params:\n",
    "    res = train(p)\n",
    "    results.append(res[\"val_accuracy\"])\n",
    "\n",
    "data = np.asarray(results)\n",
    "data = data[:, :, -1]\n",
    "avg_data = np.mean(data, axis=1).reshape(4, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2A) Activation functions vs Optimizer Algorithm\n",
    "\n",
    "For a fixed value of $N=8000$ with $\\chi=0.8$ of training, we consider different neural activation functions $\\sigma$ and different optimizer algorithms (SGD, ADAM, ADAmax, RMSprop) to be adopted in the backpropagation. Among the activation functions, we select \n",
    "\\begin{align*}\n",
    "    \\text{sigmoid}(z)&=\\frac{1}{1+e^{-z}} &\n",
    "    \\tanh(z)&=\\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}& \n",
    "    \\text{softsign}&=\\frac{z}{1+|z|}\n",
    "\\end{align*}\n",
    "\\begin{align*}\n",
    "    \\text{ReLU}&=\\text{max}(0,z)&\n",
    "    \\text{ELU}&=z\\theta(z)+k(e^{z}-1)\\theta(-z)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region [2A]: ACTIVATION FUNCTIONS VS OPTIMIZER ALGORITHMS\n",
    "\n",
    "# Dictionary of varying parameters\n",
    "vals = {\n",
    "    \"optimization\": [\"sgd\", \"adamax\", \"rmsprop\", \"adam\"],\n",
    "    \"activation\": [\"sigmoid\", \"relu\", \"tanh\", \"softsign\", \"elu\"],\n",
    "}\n",
    "\n",
    "\n",
    "# Dictionary of DNN configuration setups\n",
    "params = [\n",
    "    {\n",
    "        \"samples\": samples,\n",
    "        \"input\": {\n",
    "            \"N\": 8000,\n",
    "            \"rescale\": 50,\n",
    "            \"offset\": 0,\n",
    "            \"train_frac\": 0.8,\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"name\": \"model\",\n",
    "            \"layers\": [\n",
    "                tf.keras.layers.Dense(2, activation=activation),\n",
    "                tf.keras.layers.Dense(20, activation=activation),\n",
    "                tf.keras.layers.Dense(20, activation=activation),\n",
    "                tf.keras.layers.Dropout(0.2),\n",
    "                tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "            ],\n",
    "        },\n",
    "        \"compile\": {\n",
    "            \"loss\": \"binary_crossentropy\",\n",
    "            \"optimizer\": optimization,\n",
    "            \"metrics\": [\"accuracy\"],\n",
    "            \"steps_per_execution\": 4,\n",
    "        },\n",
    "        \"fit\": {\"epochs\": 500, \"batch_size\": 50, \"verbose\": 0},\n",
    "    }\n",
    "    for optimization, activation in product(*vals.values())\n",
    "]\n",
    "\n",
    "results = []\n",
    "for p in params:\n",
    "    res = train(p)\n",
    "    results.append(res[\"val_accuracy\"])\n",
    "\n",
    "data = np.asarray(results)\n",
    "data = data[:, :, -1]\n",
    "avg_data = np.mean(data, axis=1).reshape(4, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2A) Input Data set size $N$ vs Number of neurons per hidden layer $M$\n",
    "\n",
    "For a fixed model architecture with 2 layers and varying input data sizes $N$, we compare configurations with different number of neurons. \n",
    "\n",
    "Namely,\n",
    "\\begin{align*}\n",
    "    N&\\in\\left\\{2000,4000,8000,16000\\right\\}\\\\\n",
    "    M&\\in\\left\\{5,10,15,20,25\\right\\}\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region [2B]: INPUT SIZE N VS NUMBER OF HIDDEN NEURONS PER LAYER\n",
    "\n",
    "# Dictionary of varying parameters\n",
    "vals = {\n",
    "    \"N\": 1000 * 2 ** np.arange(1, 5),\n",
    "    # M is the number of neurons in each of the 2 Hidden Layers\n",
    "    \"M\": np.arange(5, 26, 5),\n",
    "}\n",
    "\n",
    "# Dictionary of DNN configuration setups\n",
    "params = [\n",
    "    {\n",
    "        \"samples\": samples,\n",
    "        \"input\": {\n",
    "            \"N\": N,\n",
    "            \"rescale\": 50,\n",
    "            \"offset\": 0,\n",
    "            \"train_frac\": 0.8,\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"name\": \"model\",\n",
    "            \"layers\": [\n",
    "                tf.keras.layers.Dense(2, activation=\"elu\"),\n",
    "                tf.keras.layers.Dense(M, activation=\"elu\"),\n",
    "                tf.keras.layers.Dense(M, activation=\"elu\"),\n",
    "                tf.keras.layers.Dropout(0.2),\n",
    "                tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "            ],\n",
    "        },\n",
    "        \"compile\": {\n",
    "            \"loss\": \"binary_crossentropy\",\n",
    "            \"optimizer\": \"adam\",\n",
    "            \"metrics\": [\"accuracy\"],\n",
    "            \"steps_per_execution\": 4,\n",
    "        },\n",
    "        \"fit\": {\"epochs\": 500, \"batch_size\": 50, \"verbose\": 0},\n",
    "    }\n",
    "    for N, M in product(*vals.values())\n",
    "]\n",
    "\n",
    "results = []\n",
    "for p in params:\n",
    "    res = train(p)\n",
    "    results.append(res[\"val_accuracy\"])\n",
    "\n",
    "\n",
    "data = np.asarray(results)\n",
    "data = data[:, :, -1]\n",
    "avg_data = np.mean(data, axis=1).reshape(4, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2C) Hidden Layers vs Dropout percentage\n",
    "\n",
    "We compare DNN architectures displaying different number of layers but a comparable number of parameters:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{neurons per layers} && \\text{parameters} \\\\\n",
    "(2,125,1)            && 507 \\\\\n",
    "(2,20,20,1)          && 507\\\\\n",
    "(2,16,16,10,1)       && 507\\\\\n",
    "(2,11,12,12,12,1)    && 508\\\\\n",
    "(2,11,10,10,10,10,1) && 500\\\\\n",
    "\\end{align*}\n",
    "Correspondingly, we apply dropout rate $p$ to all the hidden layers of each model configuration. In particular, $$p\\in \\left\\{0.0,0.1,0.1,0.2,0.3\\right\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region [2C]: # HIDDEN LAYERS VS DROPOUT PERCENTAGE\n",
    "def gen_arch(neurons, p_drop):\n",
    "    \"\"\"\n",
    "    This function generates the list of layers for the DNN model architecture starting from a list of neurons per layer and the dropout probabiblity per layer\n",
    "    Args:\n",
    "        neurons (list):\n",
    "            list of neurons per layer\n",
    "        p_drop (float):\n",
    "            dropout fraction applied to each hidden layer of the architecture\n",
    "\n",
    "    Returns:\n",
    "        list:\n",
    "            list of layers for the DNN architecture\n",
    "    \"\"\"\n",
    "    layers = [tf.keras.layers.Dense(2, activation=\"elu\")]\n",
    "    for n in neurons:\n",
    "        layers.append(tf.keras.layers.Dense(n, activation=\"elu\"))\n",
    "        layers.append(tf.keras.layers.Dropout(p_drop))\n",
    "    layers.append(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "    return layers\n",
    "\n",
    "\n",
    "# Dictionary of varying parameters used to plot the grid search:\n",
    "vals = {\n",
    "    \"K\": np.arange(1, 6),\n",
    "    \"dropout\": np.arange(4) / 10,\n",
    "}\n",
    "\n",
    "\n",
    "# Dictionary of DNN configuration setups\n",
    "params = [\n",
    "    {\n",
    "        \"samples\": samples,\n",
    "        \"input\": {\n",
    "            \"N\": 4000,\n",
    "            \"rescale\": 50,\n",
    "            \"offset\": 0,\n",
    "            \"train_frac\": 0.8,\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"name\": \"model\",\n",
    "            \"layers\": gen_arch(neurons, p_drop),\n",
    "        },\n",
    "        \"compile\": {\n",
    "            \"loss\": \"binary_crossentropy\",\n",
    "            \"optimizer\": \"adam\",\n",
    "            \"metrics\": [\"accuracy\"],\n",
    "            \"steps_per_execution\": 4,\n",
    "        },\n",
    "        \"fit\": {\"epochs\": 500, \"batch_size\": 50, \"verbose\": 0},\n",
    "    }\n",
    "    for p_drop in [0.0, 0.1, 0.2, 0.3]\n",
    "    for neurons in [\n",
    "        [125],\n",
    "        [20, 20],\n",
    "        [16, 16, 10],\n",
    "        [11, 12, 12, 12],\n",
    "        [11, 10, 10, 10, 10],\n",
    "    ]\n",
    "]\n",
    "\n",
    "results = []\n",
    "for p in params:\n",
    "    res = train(p)\n",
    "    results.append(res[\"val_accuracy\"])\n",
    "\n",
    "\n",
    "data = np.asarray(results)\n",
    "data = data[:, :, -1]\n",
    "avg_data = np.mean(data, axis=1).reshape(4, 5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31d7989649452b8ff5b252a3e34caf45e4ffd8a5787fe28fc2ce0245f11b7782"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
