{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "samples = 1\n",
    "\n",
    "\n",
    "def generate(\n",
    "    N, train_frac=1.0, rescale=1.0, offset=0.0, augment_frac=0.0, augment_std=0.0\n",
    "):\n",
    "    \"\"\"\n",
    "    This function generates the input data-set of N 2-dimensional points (x1,x2) in a square box and assoaciate them to a label classifying points inside/outside a triangle.\n",
    "    It defines the fraction of data devoted to the training of the network and eventually the augemtations of such data with noise normally distributed.\n",
    "    Args:\n",
    "        N (int):\n",
    "            Number of generated points\n",
    "        train_frac (float, optional):\n",
    "            Fraction of input data devoted to the network training. Defaults to 1.0.\n",
    "        rescale (float, optional):\n",
    "            Scaling factor applied to input data (x1,x2). Defaults to 1..\n",
    "        offset (float, optional):\n",
    "            Offset applied to input data (x1,x2) after rescaling. Defaults to 0..\n",
    "        augment_frac (float, optional):\n",
    "            Fraction of training data distorted with Gaussian noise without changing labels. Defaults to 0.0.\n",
    "        augment_std (float, optional):\n",
    "            std deviation of the Gaussian noise used for augmentation. Defaults to 0.0.\n",
    "    Returns:\n",
    "        (ndarray, ndarray) :\n",
    "            2D input data points (x1,x2) and the associated true 1D labels (y)\n",
    "\n",
    "    \"\"\"\n",
    "    x = np.random.random((N, 2)) * 100 - 50\n",
    "    y = (x[..., 0] > -20) & (x[..., 1] > -40) & ((x[..., 0] + x[..., 1]) < 40)\n",
    "\n",
    "    x = (x - offset) / rescale\n",
    "\n",
    "    N_train = int(N * train_frac)\n",
    "    N_augment = int(N_train * augment_frac)\n",
    "    x_train = np.pad(x[:N_train], ((0, N_augment), (0, 0)), mode=\"wrap\")\n",
    "    y_train = np.pad(y[:N_train], ((0, N_augment)), mode=\"wrap\")\n",
    "    x_train[:N_augment] += np.random.normal(0.0, augment_std, size=(N_augment, 2))\n",
    "    valid = [a[N_train:] for a in (x, y)]\n",
    "    return (x_train, y_train), valid\n",
    "\n",
    "\n",
    "def train(params):\n",
    "    \"\"\"\n",
    "    This function trains multiple times a Deep Neural Network with a chosen setup given by the parameters in 'params' and save the results of each trial into a dictionary.\n",
    "\n",
    "    Args:\n",
    "        params (dictionary of dictionaries): \n",
    "            It contains all the \"parameters\" to build a single DNN configuration.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        dictionary : \n",
    "            Results (loss function & validation accuracy) at each epoch of the DNN training, for all the investigated configurations.\n",
    "    \"\"\"\n",
    "    samples = params.setdefault(\"samples\", 1)\n",
    "    results = defaultdict(list)\n",
    "\n",
    "    for _ in range(samples):\n",
    "        # input\n",
    "        train, valid = generate(**params[\"input\"])\n",
    "\n",
    "        # model\n",
    "        mod = tf.keras.models.Sequential(**params[\"model\"])\n",
    "\n",
    "        # compile\n",
    "        mod.compile(**params[\"compile\"])\n",
    "\n",
    "        # fit\n",
    "        fit = mod.fit(*train, validation_data=valid, **params[\"fit\"])\n",
    "        for k, v in fit.history.items():\n",
    "            results[k].append(v)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def gridplot(vals, data):\n",
    "    \"\"\"\n",
    "    Plot accuracy gridsearch.\n",
    "    Before plotting averages over realizations.\n",
    "\n",
    "    Args:\n",
    "        vals (dictionary of 2 lists): it contains the values of the x and y axis of the gridsearch\n",
    "        data (ndarray): 2d array containing the validation accuracy predicted from the DNN\n",
    "\n",
    "    Returns:\n",
    "        the colormap describing the grid search of the DNN accuracy in terms two parameters\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    data = np.asarray(data)[..., -1].mean(-1)\n",
    "    data = data.reshape([len(v) for v in vals.values()])\n",
    "    sm = ax.imshow(data, origin=\"lower\", cmap=\"magma\")\n",
    "    for axis, (p, vs) in zip((ax.yaxis, ax.xaxis), vals.items()):\n",
    "        axis.set_major_locator(mpl.ticker.FixedLocator(np.arange(len(vs))))\n",
    "        axis.set_major_formatter(mpl.ticker.FixedFormatter(vs))\n",
    "        axis.set_label_text(p)\n",
    "    cbar = fig.colorbar(sm)\n",
    "    cbar.set_label(\"accuracy\")\n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region [1A]: INPUT SIZE N VS TRAINING FRACTION\n",
    "\n",
    "# Dictionary of varying parameters\n",
    "vals = {\n",
    "    \"N\": 1000 * 2 ** np.arange(5),\n",
    "    \"train_frac\": [0.6, 0.7, 0.8, 0.9],\n",
    "}\n",
    "\n",
    "# Dictionary of DNN configuration setups\n",
    "params = [\n",
    "    {\n",
    "        \"samples\": samples,\n",
    "        \"input\": {\n",
    "            \"N\": N,\n",
    "            \"rescale\": 50,\n",
    "            \"offset\": 0,\n",
    "            \"train_frac\": train_frac,\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"name\": \"model\",\n",
    "            \"layers\": [\n",
    "                tf.keras.layers.Dense(2, activation=\"relu\"),\n",
    "                tf.keras.layers.Dense(20, activation=\"relu\"),\n",
    "                tf.keras.layers.Dense(20, activation=\"relu\"),\n",
    "                tf.keras.layers.Dropout(0.2),\n",
    "                tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "            ],\n",
    "        },\n",
    "        \"compile\": {\n",
    "            \"loss\": \"binary_crossentropy\",\n",
    "            \"optimizer\": \"adam\",\n",
    "            \"metrics\": [\"accuracy\"],\n",
    "            \"steps_per_execution\": 4,\n",
    "        },\n",
    "        \"fit\": {\"epochs\": 500, \"batch_size\": 50, \"verbose\": 0},\n",
    "    }\n",
    "    for N, train_frac in product(*vals.values())\n",
    "]\n",
    "fig, ax = gridplot(vals, [train(p)[\"val_accuracy\"] for p in params])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region [1B]: TRAINING FRACTION VS AUGMENTING FRACTION\n",
    "\n",
    "# Dictionary of varying parameters\n",
    "vals = {\n",
    "    \"train_frac\": np.arange(6, 10) / 10,\n",
    "    \"augment_frac\": np.arange(4) / 3,\n",
    "}\n",
    "\n",
    "# Dictionary of DNN configuration setups\n",
    "params = [\n",
    "    {\n",
    "        \"samples\": samples,\n",
    "        \"input\": {\n",
    "            \"N\": 4000,\n",
    "            \"rescale\": 50,\n",
    "            \"offset\": 0,\n",
    "            \"train_frac\": train_frac,\n",
    "            \"augment_frac\": augment_frac,\n",
    "            \"augment_std\": 0.05773502691896,\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"name\": \"model\",\n",
    "            \"layers\": [\n",
    "                tf.keras.layers.Dense(2, activation=\"relu\"),\n",
    "                tf.keras.layers.Dense(20, activation=\"relu\"),\n",
    "                tf.keras.layers.Dense(20, activation=\"relu\"),\n",
    "                tf.keras.layers.Dropout(0.2),\n",
    "                tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "            ],\n",
    "        },\n",
    "        \"compile\": {\n",
    "            \"loss\": \"binary_crossentropy\",\n",
    "            \"optimizer\": \"adam\",\n",
    "            \"metrics\": [\"accuracy\"],\n",
    "            \"steps_per_execution\": 4,\n",
    "        },\n",
    "        \"fit\": {\"epochs\": 500, \"batch_size\": 50, \"verbose\": 0},\n",
    "    }\n",
    "    for train_frac, augment_frac in product(*vals.values())\n",
    "]\n",
    "\n",
    "results = []\n",
    "for p in params:\n",
    "    res = train(p)\n",
    "    results.append(res[\"val_accuracy\"])\n",
    "\n",
    "data = np.asarray(results)\n",
    "data = data[:, :, -1]\n",
    "avg_data = np.mean(data, axis=1).reshape(4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region [2A]: ACTIVATION FUNCTIONS VS OPTIMIZER ALGORITHMS\n",
    "\n",
    "# Dictionary of varying parameters\n",
    "vals = {\n",
    "    \"optimization\": [\"sgd\", \"adamax\", \"rmsprop\", \"adam\"],\n",
    "    \"activation\": [\"sigmoid\", \"relu\", \"tanh\", \"softsign\", \"elu\"],\n",
    "}\n",
    "\n",
    "\n",
    "# Dictionary of DNN configuration setups\n",
    "params = [\n",
    "    {\n",
    "        \"samples\": samples,\n",
    "        \"input\": {\n",
    "            \"N\": 8000,\n",
    "            \"rescale\": 50,\n",
    "            \"offset\": 0,\n",
    "            \"train_frac\": 0.8,\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"name\": \"model\",\n",
    "            \"layers\": [\n",
    "                tf.keras.layers.Dense(2, activation=activation),\n",
    "                tf.keras.layers.Dense(20, activation=activation),\n",
    "                tf.keras.layers.Dense(20, activation=activation),\n",
    "                tf.keras.layers.Dropout(0.2),\n",
    "                tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "            ],\n",
    "        },\n",
    "        \"compile\": {\n",
    "            \"loss\": \"binary_crossentropy\",\n",
    "            \"optimizer\": optimization,\n",
    "            \"metrics\": [\"accuracy\"],\n",
    "            \"steps_per_execution\": 4,\n",
    "        },\n",
    "        \"fit\": {\"epochs\": 500, \"batch_size\": 50, \"verbose\": 0},\n",
    "    }\n",
    "    for optimization, activation in product(*vals.values())\n",
    "]\n",
    "\n",
    "results = []\n",
    "for p in params:\n",
    "    res = train(p)\n",
    "    results.append(res[\"val_accuracy\"])\n",
    "\n",
    "data = np.asarray(results)\n",
    "data = data[:, :, -1]\n",
    "avg_data = np.mean(data, axis=1).reshape(4, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region [2B]: INPUT SIZE N VS NUMBER OF HIDDEN NEURONS PER LAYER\n",
    "\n",
    "# Dictionary of varying parameters\n",
    "vals = {\n",
    "    \"N\": 1000 * 2 ** np.arange(1, 5),\n",
    "    # M is the number of neurons in each of the 2 Hidden Layers\n",
    "    \"M\": np.arange(5, 26, 5),\n",
    "}\n",
    "\n",
    "# Dictionary of DNN configuration setups\n",
    "params = [\n",
    "    {\n",
    "        \"samples\": samples,\n",
    "        \"input\": {\n",
    "            \"N\": N,\n",
    "            \"rescale\": 50,\n",
    "            \"offset\": 0,\n",
    "            \"train_frac\": 0.8,\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"name\": \"model\",\n",
    "            \"layers\": [\n",
    "                tf.keras.layers.Dense(2, activation=\"elu\"),\n",
    "                tf.keras.layers.Dense(M, activation=\"elu\"),\n",
    "                tf.keras.layers.Dense(M, activation=\"elu\"),\n",
    "                tf.keras.layers.Dropout(0.2),\n",
    "                tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "            ],\n",
    "        },\n",
    "        \"compile\": {\n",
    "            \"loss\": \"binary_crossentropy\",\n",
    "            \"optimizer\": \"adam\",\n",
    "            \"metrics\": [\"accuracy\"],\n",
    "            \"steps_per_execution\": 4,\n",
    "        },\n",
    "        \"fit\": {\"epochs\": 500, \"batch_size\": 50, \"verbose\": 0},\n",
    "    }\n",
    "    for N, M in product(*vals.values())\n",
    "]\n",
    "\n",
    "results = []\n",
    "for p in params:\n",
    "    res = train(p)\n",
    "    results.append(res[\"val_accuracy\"])\n",
    "\n",
    "\n",
    "data = np.asarray(results)\n",
    "data = data[:, :, -1]\n",
    "avg_data = np.mean(data, axis=1).reshape(4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region [2C]: # HIDDEN LAYERS VS DROPOUT PERCENTAGE\n",
    "\n",
    "\n",
    "def gen_arch(neurons, p_drop):\n",
    "    \"\"\"\n",
    "    This function generates the list of layers for the DNN model architecture starting from a list of neurons per layer and the dropout probabiblity per layer\n",
    "    Args:\n",
    "        neurons (list):\n",
    "            list of neurons per layer\n",
    "        p_drop (float):\n",
    "            dropout fraction applied to each hidden layer of the architecture\n",
    "\n",
    "    Returns:\n",
    "        list:\n",
    "            list of layers for the DNN architecture\n",
    "    \"\"\"\n",
    "    layers = [tf.keras.layers.Dense(2, activation=\"elu\")]\n",
    "    for n in neurons:\n",
    "        layers.append(tf.keras.layers.Dense(n, activation=\"elu\"))\n",
    "        layers.append(tf.keras.layers.Dropout(p_drop))\n",
    "    layers.append(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "    return layers\n",
    "\n",
    "\n",
    "# Dictionary of varying parameters used to plot the grid search:\n",
    "vals = {\n",
    "    \"K\": np.arange(1, 6),\n",
    "    \"dropout\": np.arange(4) / 10,\n",
    "}\n",
    "\n",
    "\n",
    "# Dictionary of DNN configuration setups\n",
    "params = [\n",
    "    {\n",
    "        \"samples\": samples,\n",
    "        \"input\": {\n",
    "            \"N\": 4000,\n",
    "            \"rescale\": 50,\n",
    "            \"offset\": 0,\n",
    "            \"train_frac\": 0.8,\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"name\": \"model\",\n",
    "            \"layers\": gen_arch(neurons, p_drop),\n",
    "        },\n",
    "        \"compile\": {\n",
    "            \"loss\": \"binary_crossentropy\",\n",
    "            \"optimizer\": \"adam\",\n",
    "            \"metrics\": [\"accuracy\"],\n",
    "            \"steps_per_execution\": 4,\n",
    "        },\n",
    "        \"fit\": {\"epochs\": 500, \"batch_size\": 50, \"verbose\": 0},\n",
    "    }\n",
    "    for p_drop in [0.0, 0.1, 0.2, 0.3]\n",
    "    for neurons in [\n",
    "        [125],\n",
    "        [20, 20],\n",
    "        [16, 16, 10],\n",
    "        [11, 12, 12, 12],\n",
    "        [11, 10, 10, 10, 10],\n",
    "    ]\n",
    "]\n",
    "\n",
    "results = []\n",
    "for p in params:\n",
    "    res = train(p)\n",
    "    results.append(res[\"val_accuracy\"])\n",
    "\n",
    "\n",
    "data = np.asarray(results)\n",
    "data = data[:, :, -1]\n",
    "avg_data = np.mean(data, axis=1).reshape(4, 5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31d7989649452b8ff5b252a3e34caf45e4ffd8a5787fe28fc2ce0245f11b7782"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
