# from simsio import gen_configs
# import numpy as np
# params = {
#     "rescale": np.logspace(-1, 3, 5),
#     "lr": np.logspace(-3.5, -1.5, 5),
# }
# gen_configs("template", params, "II/3A")
===:
  template: |
    n$enum:
      <<<: common
      input: {rescale: $rescale}
      compile: {optimizer: {config: {learning_rate: $lr}}}
  common:
    samples: 100
    input:
      N: 4000
      train_frac: 0.8
      offset: 0
    model:
      layers:
      - tf.keras.layers.Dense(2, activation="elu")
      - tf.keras.layers.Dense(20, activation="elu")
      - tf.keras.layers.Dense(20, activation="elu")
      - tf.keras.layers.Dropout(0.2)
      - tf.keras.layers.Dense(1, activation="sigmoid")
    compile:
      loss: binary_crossentropy
      optimizer:
        class_name: adam
      metrics: [accuracy]
      steps_per_execution: 4
    fit:
      epochs: 500
      batch_size: 50
      verbose: 0
9e2436d2a72111ec98dcfa163e66c7df:
  <<<: common
  input: {rescale: 1.0}
  compile: {optimizer: {config: {learning_rate: 0.00031622776601683794}}}
a59f0e8ca72111ecb7f2fa163e209104:
  <<<: common
  input: {rescale: 1.0}
  compile: {optimizer: {config: {learning_rate: 0.001}}}
a6f47362a72111ecb5d7fa163e209104:
  <<<: common
  input: {rescale: 1.0}
  compile: {optimizer: {config: {learning_rate: 0.0031622776601683794}}}
9d2e6428a72111ec9bd9fa163e595e79:
  <<<: common
  input: {rescale: 1.0}
  compile: {optimizer: {config: {learning_rate: 0.01}}}
b1a73fe2a72111ecafa2fa163ea6f025:
  <<<: common
  input: {rescale: 1.0}
  compile: {optimizer: {config: {learning_rate: 0.03162277660168379}}}
9fb5cb50a72111ecafdcfa163e209104:
  <<<: common
  input: {rescale: 10.0}
  compile: {optimizer: {config: {learning_rate: 0.00031622776601683794}}}
9e242b60a72111ecadd6fa163e66c7df:
  <<<: common
  input: {rescale: 10.0}
  compile: {optimizer: {config: {learning_rate: 0.001}}}
9d72bdeea72111ecb67cfa163efacce8:
  <<<: common
  input: {rescale: 10.0}
  compile: {optimizer: {config: {learning_rate: 0.0031622776601683794}}}
9d72b632a72111eca4defa163efacce8:
  <<<: common
  input: {rescale: 10.0}
  compile: {optimizer: {config: {learning_rate: 0.01}}}
a38f6704a72111ecbb79fa163ea6f025:
  <<<: common
  input: {rescale: 10.0}
  compile: {optimizer: {config: {learning_rate: 0.03162277660168379}}}
9d7566caa72111ec9dedfa163e04b9dc:
  <<<: common
  input: {rescale: 100.0}
  compile: {optimizer: {config: {learning_rate: 0.00031622776601683794}}}
9fb5fc38a72111eca9d0fa163e209104:
  <<<: common
  input: {rescale: 100.0}
  compile: {optimizer: {config: {learning_rate: 0.001}}}
9d748822a72111ec9637fa163efacce8:
  <<<: common
  input: {rescale: 100.0}
  compile: {optimizer: {config: {learning_rate: 0.0031622776601683794}}}
a38e43c4a72111ec9187fa163ea6f025:
  <<<: common
  input: {rescale: 100.0}
  compile: {optimizer: {config: {learning_rate: 0.01}}}
9d75950aa72111eca69efa163e04b9dc:
  <<<: common
  input: {rescale: 100.0}
  compile: {optimizer: {config: {learning_rate: 0.03162277660168379}}}
e7eb26e8a76411ecbffcfa163e595e79:
  <<<: common
  input: {rescale: 1000.0}
  compile: {optimizer: {config: {learning_rate: 0.00031622776601683794}}}
0c868fb0a76511eca05cfa163e209104:
  <<<: common
  input: {rescale: 1000.0}
  compile: {optimizer: {config: {learning_rate: 0.001}}}
170842b2a76511ecb052fa163e595e79:
  <<<: common
  input: {rescale: 1000.0}
  compile: {optimizer: {config: {learning_rate: 0.0031622776601683794}}}
1b8fffaaa76511ec85e7fa163e209104:
  <<<: common
  input: {rescale: 1000.0}
  compile: {optimizer: {config: {learning_rate: 0.01}}}
41e623c8a76511eca8cffa163e04b9dc:
  <<<: common
  input: {rescale: 1000.0}
  compile: {optimizer: {config: {learning_rate: 0.03162277660168379}}}




















































